{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59da6d66",
      "metadata": {
        "id": "59da6d66",
        "outputId": "59613987-0a27-4f5f-de21-affc8d3b4649"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JAVA_HOME = /usr/lib/jvm/temurin-11-jdk-amd64\n",
            "SPARK_HOME = /usr/lib/spark\n",
            "PYSPARK_SUBMIT_ARGS = --deploy-mode client pyspark-shell\n"
          ]
        }
      ],
      "source": [
        "# Check important enviroment variables\n",
        "import os\n",
        "print(\"JAVA_HOME =\", os.environ.get(\"JAVA_HOME\"))\n",
        "print(\"SPARK_HOME =\", os.environ.get(\"SPARK_HOME\"))\n",
        "print(\"PYSPARK_SUBMIT_ARGS =\", os.environ.get(\"PYSPARK_SUBMIT_ARGS\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2bdf344",
      "metadata": {
        "id": "d2bdf344",
        "outputId": "1e663efa-7721-4e79-da1c-4232058253c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/05/15 19:56:35 INFO SparkEnv: Registering MapOutputTracker\n",
            "25/05/15 19:56:35 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/05/15 19:56:35 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/05/15 19:56:35 INFO SparkEnv: Registering OutputCommitCoordinator\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "spark.version\n",
        "\n",
        "from pyspark.sql.functions import (\n",
        "    col, when, lead, input_file_name, regexp_extract, to_date,\n",
        "    udf, sum as spark_sum, mean, stddev, min, max, count, sum\n",
        ")\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "from pyspark.sql.types import (\n",
        "    StructType, StructField, StringType, IntegerType, DoubleType, DateType, LongType\n",
        ")\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.pipeline import PipelineModel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb5e24e8",
      "metadata": {
        "id": "fb5e24e8"
      },
      "source": [
        "# Financial Article Sentiment Analysis\n",
        "## Student: Ashley Sun\n",
        "\n",
        "On April 7th, Monday, a false headline about President Trump considering a 90-day pause on the tariffs led to a swift surge of the S&P 500, adding trillions in market value within minutes, then crashing down rapidly after the White House refuted the report. This incident highlights the need for a performant live sentiment analysis system to make trading decisions. For the project, I built a scalable ML pipeline using big data tools such as Google Cloud Data Ingestion, Hadoop, HDFS, Spark, and Spark MLlib. I trained my own financial news sentiment analysis model from scratch and tested the model against historical pricing data.\n",
        "\n",
        "## Tasks\n",
        "- Ingest financial news, historical price data, and sentiment word lists at scale\n",
        "- Clean, format, and efficiently store the input\n",
        "- Featurize and label the articles\n",
        "- Train and test our own sentiment analysis model on dated financial news\n",
        "- Efficiently manage and store intermediate results such as models\n",
        "- Combine sentiment prediction with historical financial data for evaluation\n",
        "- Efficiently evaluate and backtest the strategy\n",
        "\n",
        "## Data Sources:\n",
        "- Nasdaq Financial News: ~26GB, 15 millions dated financial articles\n",
        "- Nasdaq Historical Prices: ~ 4700 csv files. Each file contains historical prices of a symbol.\n",
        "- Loughran-McDonald Sentiment Word List: sentiment word list focusing on financial news\n",
        "- Liu Bing Sentiment Word List: supplemental generic sentiment word list\n",
        "\n",
        "## Challenges and Solutions\n",
        "- Storing and processing data at scale is difficult.\n",
        "    - Solution: I utilized Google Data Ingestion and HDFS on Dataproc to manage all my data. I manually uploaded the data files to Ingestion portal, logged into Dataproc and copied it to HDFS.\n",
        "- NYU Google Cloud Dataproc does not have many ML libraries such as pytorch, Scikit Learn, or SparkNLP.\n",
        "    - Solution: I built my own ML model with very basic libraries such as SparkML.\n",
        "- Our JupyterLab (the one we used for homework) is not on Dataproc and cannot handle the amount of data required.\n",
        "    - Solution: I ran a Jupyter Notebook with PySpark on DataProc. However it is still quite challenging due to environmental setup.\n",
        "        - Solution on running Jupyter Notebook on NYU Dataproc:\n",
        "\n",
        "        ```\n",
        "        export JAVA_HOME=/usr/lib/jvm/temurin-11-jdk-amd64\n",
        "        export SPARK_HOME=/usr/lib/spark\n",
        "        export PYSPARK_PYTHON=python3\n",
        "        export PYSPARK_DRIVER_PYTHON=jupyter\n",
        "        export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\"\n",
        "        unset SPARK_SUBMIT_OPTIONS\n",
        "        unset PYSPARK_SUBMIT_ARGS\n",
        "        export PYSPARK_SUBMIT_ARGS=\"--deploy-mode client pyspark-shell\"\n",
        "        jupyter notebook\n",
        "        ```\n",
        "        - To visit the notebook from my laptop, I installed GCloud and did a port forwarding:\n",
        "        ```\n",
        "        gcloud compute ssh nyu-dataproc-m --project hpc-dataproc-19b8 --zone us-central1-f -- -N -L 8888:localhost:8888\n",
        "        ```\n",
        "Sources: https://sites.google.com/nyu.edu/nyu-hpc/hpc-systems/cloud-computing/dataproc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44bc07f3",
      "metadata": {
        "id": "44bc07f3"
      },
      "source": [
        "# Raw Data Processing\n",
        "\n",
        "Raw artical data is downloaded from https://huggingface.co/datasets/Zihan1004/FNSPID/resolve/main/Stock_news/nasdaq_exteral_data.csv to my local laptop. Then I uploaded to https://dataproc.hpc.nyu.edu/ingest Data Ingestion Console. The data there is stored in Google Cloud Storage and it's temporary. To use the data in Spark in this notebook, I copied the data to hdfs with hadoop fs -cp gs://nyu-dataproc-hdfs-ingest/****/nasdaq_exteral_data.csv.\n",
        "\n",
        "Even though it is a CSV file, the format is bad for Spark. Instead of 1 record per row, the Article column spans multiple line. Each article is inside of \"..\". This is bad for parallelizing task because in order to find the next record, we have to parse each line until a \". We cannot split the file arbitrariy since we might cut a record in half. As we see later, this dataset has **15,549,299** records. Proper preprocessing and storage are neccesary.\n",
        "\n",
        "To address this, I'm loading the raw file once with proper parsing and storing the cleaned result in Spark Parquet format on HDFS. Parquet is a columnar storage format that is fast for Spark.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30f16a6d",
      "metadata": {
        "id": "30f16a6d",
        "outputId": "d8ef4746-f3db-4a38-cbe1-78638b15984f",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "schema = StructType([\n",
        "    StructField(\"Unnamed: 0\", IntegerType(), True),        # likely an index column\n",
        "    StructField(\"Date\", DateType(), True),                 # e.g., 2023-05-01\n",
        "    StructField(\"Article_title\", StringType(), True),\n",
        "    StructField(\"Stock_symbol\", StringType(), True),\n",
        "    StructField(\"Url\", StringType(), True),\n",
        "    StructField(\"Publisher\", StringType(), True),\n",
        "    StructField(\"Author\", StringType(), True),\n",
        "    StructField(\"Article\", StringType(), True),\n",
        "    StructField(\"Lsa_summary\", StringType(), True),\n",
        "    StructField(\"Luhn_summary\", StringType(), True),\n",
        "    StructField(\"Textrank_summary\", StringType(), True),\n",
        "    StructField(\"Lexrank_summary\", StringType(), True),\n",
        "])\n",
        "\n",
        "# Intentionally commented out don't run this again\n",
        "\n",
        "# # The Aritcl fields are all multi-line\n",
        "# df = spark.read.csv(\n",
        "#     '/user/***/nasdaq_exteral_data.csv',\n",
        "#     header=True,\n",
        "#     multiLine=True,\n",
        "#     escape='\"',\n",
        "#     quote='\"',\n",
        "#     mode='DROPMALFORMED'\n",
        "# )\n",
        "\n",
        "# # Convert once to parquet so we can read it out faster in the future\n",
        "# df.select(\"Date\", \"Article_title\", \"Stock_symbol\", \"Article\").repartition(8).write.mode(\"overwrite\").parquet('/user/****/nasdaq_parquet/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66dfc41b",
      "metadata": {
        "id": "66dfc41b",
        "outputId": "12df1468-f260-48c1-b2dd-18b239b4a93c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15549299\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Article_title</th>\n",
              "      <th>Stock_symbol</th>\n",
              "      <th>Article</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2007-01-11 00:26:00 UTC</td>\n",
              "      <td>ANALYSIS-US Mideast peace bid faces many obsta...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2014-12-03 00:00:00 UTC</td>\n",
              "      <td>Equifax Canada Reports Consumer Debt Grows to ...</td>\n",
              "      <td>EFX</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-12-01 00:00:00 UTC</td>\n",
              "      <td>Силуанов назвал главную задачу обновленной нал...</td>\n",
              "      <td>None</td>\n",
              "      <td>Налоговая система России после 2018 года должн...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2023-10-09 00:00:00 UTC</td>\n",
              "      <td>IJR, ONTO, FN, RMBS: ETF Outflow Alert</td>\n",
              "      <td>GSEE</td>\n",
              "      <td>Looking today at week-over-week shares outstan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2017-12-06 00:00:00 UTC</td>\n",
              "      <td>Американские учителя рассказали о неумеющих чи...</td>\n",
              "      <td>None</td>\n",
              "      <td>Учителя и учащиеся некоторых школ в Калифорнии...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2021-08-11 00:00:00 UTC</td>\n",
              "      <td>After Hours Most Active for Aug 11, 2021 : CLO...</td>\n",
              "      <td>HBAN</td>\n",
              "      <td>The NASDAQ 100 After Hours Indicator is down -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2016-01-09 03:54:00 UTC</td>\n",
              "      <td>Asia Gold-India back at discount; yuan slide s...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2013-05-14 07:00:00 UTC</td>\n",
              "      <td>Event Alert: Kinaxis to Host Session at the Ga...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2007-11-09 05:53:00 UTC</td>\n",
              "      <td>Cruise, producer back with UA's \"Lions for Lambs\"</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2013-09-03 00:00:00 UTC</td>\n",
              "      <td>Weekly Economic Overview (September 3 - 6, 2013).</td>\n",
              "      <td>GLD</td>\n",
              "      <td>Last week news of US threat of attack on Syria...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Date                                      Article_title  \\\n",
              "0  2007-01-11 00:26:00 UTC  ANALYSIS-US Mideast peace bid faces many obsta...   \n",
              "1  2014-12-03 00:00:00 UTC  Equifax Canada Reports Consumer Debt Grows to ...   \n",
              "2  2016-12-01 00:00:00 UTC  Силуанов назвал главную задачу обновленной нал...   \n",
              "3  2023-10-09 00:00:00 UTC             IJR, ONTO, FN, RMBS: ETF Outflow Alert   \n",
              "4  2017-12-06 00:00:00 UTC  Американские учителя рассказали о неумеющих чи...   \n",
              "5  2021-08-11 00:00:00 UTC  After Hours Most Active for Aug 11, 2021 : CLO...   \n",
              "6  2016-01-09 03:54:00 UTC  Asia Gold-India back at discount; yuan slide s...   \n",
              "7  2013-05-14 07:00:00 UTC  Event Alert: Kinaxis to Host Session at the Ga...   \n",
              "8  2007-11-09 05:53:00 UTC  Cruise, producer back with UA's \"Lions for Lambs\"   \n",
              "9  2013-09-03 00:00:00 UTC  Weekly Economic Overview (September 3 - 6, 2013).   \n",
              "\n",
              "  Stock_symbol                                            Article  \n",
              "0         None                                               None  \n",
              "1          EFX                                               None  \n",
              "2         None  Налоговая система России после 2018 года должн...  \n",
              "3         GSEE  Looking today at week-over-week shares outstan...  \n",
              "4         None  Учителя и учащиеся некоторых школ в Калифорнии...  \n",
              "5         HBAN  The NASDAQ 100 After Hours Indicator is down -...  \n",
              "6         None                                               None  \n",
              "7         None                                               None  \n",
              "8         None                                               None  \n",
              "9          GLD  Last week news of US threat of attack on Syria...  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the data from pre-processed parquet saved on my HDFS\n",
        "df = spark.read.parquet('/user/****/nasdaq_parquet/')\n",
        "print(df.count())\n",
        "df.limit(10).toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab0e4ee9",
      "metadata": {
        "id": "ab0e4ee9"
      },
      "source": [
        "# Load Sentiment Words for model training\n",
        "\n",
        "In order to train my own model, I need to know what words are positive and what are negative. Instead of enumerating it myself, here is a well-known dataset for determining the sentiment of a word. This dataset is geared towards financial articles.\n",
        "https://sraf.nd.edu/loughranmcdonald-master-dictionary/. This is also uploaded to data injest then I copied it to HDFS. This file is 86k lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e93e4149",
      "metadata": {
        "id": "e93e4149",
        "outputId": "eb090323-1d39-408e-c496-462242c16606"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Load csv from hdfs\n",
        "lm_df = spark.read.csv(\n",
        "    \"/user/****/Loughran-McDonald_MasterDictionary_1993-2024.csv\",\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d94f343f",
      "metadata": {
        "id": "d94f343f",
        "outputId": "9c4ca920-1a05-4313-dce4-2f0bc0984037"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Word: string (nullable = true)\n",
            " |-- Seq_num: integer (nullable = true)\n",
            " |-- Word Count: integer (nullable = true)\n",
            " |-- Word Proportion: double (nullable = true)\n",
            " |-- Average Proportion: double (nullable = true)\n",
            " |-- Std Dev: double (nullable = true)\n",
            " |-- Doc Count: integer (nullable = true)\n",
            " |-- Negative: integer (nullable = true)\n",
            " |-- Positive: integer (nullable = true)\n",
            " |-- Uncertainty: integer (nullable = true)\n",
            " |-- Litigious: integer (nullable = true)\n",
            " |-- Strong_Modal: integer (nullable = true)\n",
            " |-- Weak_Modal: integer (nullable = true)\n",
            " |-- Constraining: integer (nullable = true)\n",
            " |-- Complexity: integer (nullable = true)\n",
            " |-- Syllables: integer (nullable = true)\n",
            " |-- Source: string (nullable = true)\n",
            "\n",
            "86553\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>Seq_num</th>\n",
              "      <th>Word Count</th>\n",
              "      <th>Word Proportion</th>\n",
              "      <th>Average Proportion</th>\n",
              "      <th>Std Dev</th>\n",
              "      <th>Doc Count</th>\n",
              "      <th>Negative</th>\n",
              "      <th>Positive</th>\n",
              "      <th>Uncertainty</th>\n",
              "      <th>Litigious</th>\n",
              "      <th>Strong_Modal</th>\n",
              "      <th>Weak_Modal</th>\n",
              "      <th>Constraining</th>\n",
              "      <th>Complexity</th>\n",
              "      <th>Syllables</th>\n",
              "      <th>Source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AARDVARK</td>\n",
              "      <td>1</td>\n",
              "      <td>755</td>\n",
              "      <td>2.955070e-08</td>\n",
              "      <td>1.945421e-08</td>\n",
              "      <td>4.078069e-06</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>12of12inf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AARDVARKS</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1.174200e-10</td>\n",
              "      <td>8.060019e-12</td>\n",
              "      <td>8.919011e-09</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>12of12inf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ABACI</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>3.522600e-10</td>\n",
              "      <td>1.089343e-10</td>\n",
              "      <td>5.105359e-08</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>12of12inf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ABACK</td>\n",
              "      <td>4</td>\n",
              "      <td>29</td>\n",
              "      <td>1.135060e-09</td>\n",
              "      <td>6.197922e-10</td>\n",
              "      <td>1.539279e-07</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>12of12inf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ABACUS</td>\n",
              "      <td>5</td>\n",
              "      <td>9620</td>\n",
              "      <td>3.765268e-07</td>\n",
              "      <td>3.825261e-07</td>\n",
              "      <td>3.421836e-05</td>\n",
              "      <td>1295</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>12of12inf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ABACUSES</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>12of12inf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ABAFT</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>1.565600e-10</td>\n",
              "      <td>2.144787e-11</td>\n",
              "      <td>2.373367e-08</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>12of12inf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>ABALONE</td>\n",
              "      <td>8</td>\n",
              "      <td>149</td>\n",
              "      <td>5.831860e-09</td>\n",
              "      <td>4.729504e-09</td>\n",
              "      <td>1.031859e-06</td>\n",
              "      <td>52</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>12of12inf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>ABALONES</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>3.914000e-11</td>\n",
              "      <td>7.715206e-11</td>\n",
              "      <td>8.537449e-08</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>12of12inf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ABANDON</td>\n",
              "      <td>10</td>\n",
              "      <td>154158</td>\n",
              "      <td>6.033745e-06</td>\n",
              "      <td>4.824004e-06</td>\n",
              "      <td>3.261271e-05</td>\n",
              "      <td>76324</td>\n",
              "      <td>2009</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>12of12inf</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Word  Seq_num  Word Count  Word Proportion  Average Proportion  \\\n",
              "0   AARDVARK        1         755     2.955070e-08        1.945421e-08   \n",
              "1  AARDVARKS        2           3     1.174200e-10        8.060019e-12   \n",
              "2      ABACI        3           9     3.522600e-10        1.089343e-10   \n",
              "3      ABACK        4          29     1.135060e-09        6.197922e-10   \n",
              "4     ABACUS        5        9620     3.765268e-07        3.825261e-07   \n",
              "5   ABACUSES        6           0     0.000000e+00        0.000000e+00   \n",
              "6      ABAFT        7           4     1.565600e-10        2.144787e-11   \n",
              "7    ABALONE        8         149     5.831860e-09        4.729504e-09   \n",
              "8   ABALONES        9           1     3.914000e-11        7.715206e-11   \n",
              "9    ABANDON       10      154158     6.033745e-06        4.824004e-06   \n",
              "\n",
              "        Std Dev  Doc Count  Negative  Positive  Uncertainty  Litigious  \\\n",
              "0  4.078069e-06        140         0         0            0          0   \n",
              "1  8.919011e-09          1         0         0            0          0   \n",
              "2  5.105359e-08          7         0         0            0          0   \n",
              "3  1.539279e-07         28         0         0            0          0   \n",
              "4  3.421836e-05       1295         0         0            0          0   \n",
              "5  0.000000e+00          0         0         0            0          0   \n",
              "6  2.373367e-08          1         0         0            0          0   \n",
              "7  1.031859e-06         52         0         0            0          0   \n",
              "8  8.537449e-08          1         0         0            0          0   \n",
              "9  3.261271e-05      76324      2009         0            0          0   \n",
              "\n",
              "   Strong_Modal  Weak_Modal  Constraining  Complexity  Syllables     Source  \n",
              "0             0           0             0           0          2  12of12inf  \n",
              "1             0           0             0           0          2  12of12inf  \n",
              "2             0           0             0           0          3  12of12inf  \n",
              "3             0           0             0           0          2  12of12inf  \n",
              "4             0           0             0           0          3  12of12inf  \n",
              "5             0           0             0           0          4  12of12inf  \n",
              "6             0           0             0           0          2  12of12inf  \n",
              "7             0           0             0           0          4  12of12inf  \n",
              "8             0           0             0           0          4  12of12inf  \n",
              "9             0           0             0           0          3  12of12inf  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lm_df.printSchema()\n",
        "print(lm_df.count()) # This is small so we can do this\n",
        "lm_df.limit(10).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d9f942f",
      "metadata": {
        "id": "3d9f942f"
      },
      "outputs": [],
      "source": [
        "# Keep only relevant columns\n",
        "basic_cols = [\n",
        "    \"Word\",\n",
        "    \"Word Proportion\",\n",
        "    \"Average Proportion\",\n",
        "]\n",
        "category_cols = [\n",
        "    \"Negative\",\n",
        "    \"Positive\",\n",
        "    \"Uncertainty\",\n",
        "    \"Litigious\",\n",
        "    \"Strong_Modal\",\n",
        "    \"Weak_Modal\",\n",
        "    \"Constraining\"\n",
        "]\n",
        "selected_cols = basic_cols + category_cols\n",
        "\n",
        "lm_df_filtered = lm_df.select(*selected_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6e7ceb9",
      "metadata": {
        "id": "c6e7ceb9"
      },
      "source": [
        "## Converting from versioned category value to True and False\n",
        "From the website, it states:\n",
        "\"The sentiment categories are: negative, positive, uncertainty, litigious, strong modal, weak modal, and constraining. The sentiment words are flagged with a number indicating the year in which they were added to the list. Note: A year preceded by a negative sign indicates the year/version when the word was removed from the sentiment category.\"\n",
        "\n",
        "The end goal is to have a list of words for each category, so first we convert the versioned to true false"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d8ba3dd",
      "metadata": {
        "id": "1d8ba3dd"
      },
      "outputs": [],
      "source": [
        "category_cols = [\n",
        "    \"Negative\",\n",
        "    \"Positive\",\n",
        "    \"Uncertainty\",\n",
        "    \"Litigious\",\n",
        "    \"Strong_Modal\",\n",
        "    \"Weak_Modal\",\n",
        "    \"Constraining\"\n",
        "]\n",
        "\n",
        "# For integer-type version columns\n",
        "for col_name in category_cols:\n",
        "    lm_df_filtered = lm_df_filtered.withColumn(\n",
        "        col_name,\n",
        "        when(col(col_name).isNotNull() & (col(col_name) > 0), True).otherwise(False)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a58cd1e4",
      "metadata": {
        "id": "a58cd1e4",
        "outputId": "5e893763-f1d4-4638-a8a9-17df3c8b9108",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of neg word set: 3245\n",
            "Size of pos word set: 389\n"
          ]
        }
      ],
      "source": [
        "neg_words = lm_df_filtered.filter((col(\"Negative\") == True) | \\\n",
        "                                  (col(\"Litigious\") == True) | \\\n",
        "                                  (col(\"Constraining\") == True) ).select(\"Word\").rdd.flatMap(lambda x: x).collect()\n",
        "neg_regex = \"(?i)\\\\b(\" + \"|\".join([w.lower() for w in neg_words]) + \")\\\\b\"\n",
        "neg_set = set(neg_words)\n",
        "neg_size = len(neg_set)\n",
        "print(f'Size of neg word set: {neg_size}')\n",
        "\n",
        "\n",
        "pos_words = lm_df_filtered.filter((col(\"Positive\") == True) | \\\n",
        "                                  (col(\"Strong_Modal\") == True) | \\\n",
        "                                  (col(\"Weak_Modal\") == True)).select(\"Word\").rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "pos_regex = \"(?i)\\\\b(\" + \"|\".join([w.lower() for w in pos_words]) + \")\\\\b\"\n",
        "pos_set = set(pos_words)\n",
        "pos_size = len(pos_set)\n",
        "print(f'Size of pos word set: {pos_size}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7e912d3",
      "metadata": {
        "id": "a7e912d3"
      },
      "source": [
        "## Adding Liu Bing Word Sets for Balancing\n",
        "Noticed that this opinion word list is very imbalanced. I found another list called https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html which contains more generic positive and negative words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b816564",
      "metadata": {
        "id": "4b816564",
        "outputId": "4ae3ad4c-ed82-4938-b028-de1b2bd25c6c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 2006 positive and 4783 negative words.\n"
          ]
        }
      ],
      "source": [
        "# Load and filter lines (skip empty and comment lines)\n",
        "pos_rdd = sc.textFile(\"/user/****/positive-words.txt\") \\\n",
        "            .filter(lambda line: line.strip() and not line.strip().startswith(\";\")) \\\n",
        "            .map(lambda word: word.strip().upper())\n",
        "\n",
        "neg_rdd = sc.textFile(\"/user/****/negative-words.txt\") \\\n",
        "            .filter(lambda line: line.strip() and not line.strip().startswith(\";\")) \\\n",
        "            .map(lambda word: word.strip().upper())\n",
        "\n",
        "\n",
        "# Example usage\n",
        "pos_set_2 = set(pos_rdd.collect())\n",
        "neg_set_2 = set(neg_rdd.collect())\n",
        "\n",
        "print(f\"Loaded {len(pos_set_2)} positive and {len(neg_set_2)} negative words.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7da83c7",
      "metadata": {
        "id": "e7da83c7",
        "outputId": "8915d1e7-25b6-4146-9ebf-66021f597494"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of pos word set: 2204\n",
            "Size of neg word set: 7167\n"
          ]
        }
      ],
      "source": [
        "# Combine both sets\n",
        "pos_set = pos_set.union(pos_set_2)\n",
        "neg_set = neg_set.union(neg_set_2)\n",
        "pos_size = len(pos_set)\n",
        "neg_size = len(neg_set)\n",
        "print(f'Size of pos word set: {pos_size}')\n",
        "print(f'Size of neg word set: {neg_size}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb8ed73b",
      "metadata": {
        "id": "fb8ed73b"
      },
      "source": [
        "# Limited Sample Size Test\n",
        "\n",
        "We will first test our pipelines with a small sample size of 10k articles. In this debugging pipeline, I'm calling show() a lot which triggers evaluation. This is slow and impractical on large sample size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35de1114",
      "metadata": {
        "id": "35de1114",
        "outputId": "721e8a3b-eab9-4e88-e02a-b09a3fa16242"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Reduce sample size and drop empty\n",
        "df = spark.read.parquet('/user/****/nasdaq_parquet/')\n",
        "\n",
        "# Limit size and filter empty,null articles\n",
        "df_filtered = (\n",
        "    df.limit(10000)\n",
        "      .dropna(subset=[\"Article\", \"Stock_symbol\"])  # removes nulls\n",
        "      .filter(\n",
        "          (col(\"Article\").isNotNull()) & (col(\"Article\") != \"\") &\n",
        "          (col(\"Stock_symbol\").isNotNull()) & (col(\"Stock_symbol\") != \"\")\n",
        "      )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db9a55f0",
      "metadata": {
        "id": "db9a55f0"
      },
      "source": [
        "## Labeling the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8c79613",
      "metadata": {
        "id": "a8c79613"
      },
      "source": [
        "I first tried to use Spark regex to match against my word lists, but this doesn't work if both word lists are long. When an article matches something in both word lists, the pos always wins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f1a3c0a",
      "metadata": {
        "id": "3f1a3c0a",
        "outputId": "3d73a95c-9ef3-4e9c-8db4-0aa834210548"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            "[Stage 403:============================================>          (55 + 2) / 68]\r"
          ]
        }
      ],
      "source": [
        "# This doesn't work when there are a lot of words in each category\n",
        "# df_labeled = df_filtered.withColumn(\n",
        "#     \"label\",\n",
        "#     when(col(\"Article\").rlike(pos_regex), 1)\n",
        "#     .when(col(\"Article\").rlike(neg_regex), 0)\n",
        "#     .otherwise(None)\n",
        "# ).filter(col(\"label\").isNotNull())  # Keep only labeled rows"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a092ec0",
      "metadata": {
        "id": "9a092ec0"
      },
      "source": [
        "Therefore I am writing my own udf to compute a score base on count. Since the 2 sets of words have drastically different counts, I'm normalizing by their respective total count."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0857d71e",
      "metadata": {
        "id": "0857d71e"
      },
      "outputs": [],
      "source": [
        "def score_by_count(text):\n",
        "    import builtins\n",
        "    if text is None:\n",
        "        return 0\n",
        "    words = text.lower().split()\n",
        "    # usinging builtins.sum because I imported PySpark sum at the top\n",
        "    pos_count = builtins.sum(1 for w in words if w.upper() in pos_set)\n",
        "    neg_count = builtins.sum(1 for w in words if w.upper() in neg_set)\n",
        "    pos_score = pos_count / pos_size\n",
        "    neg_score = neg_count / neg_size\n",
        "    return pos_score - neg_score\n",
        "\n",
        "score_udf = udf(score_by_count, DoubleType())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12cc96e8",
      "metadata": {
        "id": "12cc96e8"
      },
      "outputs": [],
      "source": [
        "df_filtered = df_filtered.withColumn(\"scaled_sentiment_score\", score_udf(col(\"Article_title\"))) \\\n",
        "       .withColumn(\"label\", when(col(\"scaled_sentiment_score\") > 0, 2)\n",
        "                           .when(col(\"scaled_sentiment_score\") == 0, 1)\n",
        "                           .when(col(\"scaled_sentiment_score\") < 0, 0)\n",
        "                           .otherwise(None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "330a177f",
      "metadata": {
        "id": "330a177f",
        "outputId": "40bcaade-a2da-4c9c-8884-be03ed6cdfa3",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 403:============>  (57 + 2) / 68][Stage 410:>                (0 + 0) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+----------------------+\n",
            "|       Article_title|scaled_sentiment_score|\n",
            "+--------------------+----------------------+\n",
            "|Verizon Is Growin...|  -1.39528394028184...|\n",
            "|New Media Investm...|  4.537205081669691...|\n",
            "|Tech Today: Broad...|  4.537205081669691...|\n",
            "|Atlantica Sustain...|  4.537205081669691...|\n",
            "|Is SPDR MSCI EAFE...|  9.074410163339383E-4|\n",
            "|Rio Tinto (RIO) I...|  4.537205081669691...|\n",
            "|Silver Weekly Pri...|  -2.79056788056369...|\n",
            "|Wall Street drops...|  -1.39528394028184...|\n",
            "|Why Hecla Mining ...|  3.141921141387844E-4|\n",
            "|Natural Gas Price...|  4.537205081669691...|\n",
            "+--------------------+----------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            "[Stage 403:============>  (58 + 1) / 68][Stage 410:>                (0 + 1) / 1]\r\n",
            "\r\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "#Inspect some non-zero ones\n",
        "df_filtered.filter(col(\"scaled_sentiment_score\") != 0.0).select(\"Article_title\", \"scaled_sentiment_score\").show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63434d9d",
      "metadata": {
        "id": "63434d9d"
      },
      "source": [
        "## Article Featurization with TF-IDF\n",
        "We use Term Frequency - Inverse Document Frequence to featurize each Article. It transforms each Article to a feature (numeric) vector. The ith value in the vector is the TF-IDF score of the ith word in the document, which signifies the importance of the word. TF-IDF has 2 distinct steps:\n",
        "\n",
        "1. Term Frequency: TF(term, doc) = count of term in doc / total terms in doc: this is the frequency of the term within the current article. This represents how important a word is in a document. This step is easily parallelizable. Each Article can be proccessed independently (on different Spark nodes).\n",
        "\n",
        "2. Inverse Document Frequency: IDF(term) = log( total number of articles / (1 + number of articles containing term). This step downweight words that are frequent across all documents such as \"the\", and \"a\" since they provide very little value.\n",
        "\n",
        "Ideally, our feature vector is long enough so that every word in our sample set can be accounted for. In practice, we limit our vector size to numFeatures. We hash each word to get the corresponding index.\n",
        "\n",
        "## Logistic Regression on Categorical Label\n",
        "\n",
        "In the previous steps, we labeld each article 0,1,2 (negative, neutral, positive) depending on the sentiment score and we created feature vector with TF-IDF. The final step is to choose a model. I'm using Logistic Regression (https://spark.apache.org/docs/latest//ml-classification-regression.html#logistic-regression) which is a very common supervised machine learning algorithm for classification. Specifically, I'm using Multinomial Logistic Regression since I have 3 categories.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b81ee4a",
      "metadata": {
        "id": "9b81ee4a"
      },
      "outputs": [],
      "source": [
        "# Tokenize the articles (like homework assignment)\n",
        "# https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Tokenizer.html\n",
        "tokenizer = Tokenizer(inputCol=\"Article\", outputCol=\"words\")\n",
        "\n",
        "# Remove stop words like \"the\", \"is\", \"and\", \"a\", \"in\", \"to\", \"for\", \"on\", \"of\", \"with\"\n",
        "# https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.StopWordsRemover.html\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "\n",
        "# TF discussed above\n",
        "tf = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=200)\n",
        "\n",
        "# IDF discussed above\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "\n",
        "# Logistic Multi-label regression\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", family=\"multinomial\", maxIter=100)\n",
        "\n",
        "pipeline = Pipeline(stages=[tokenizer, remover, tf, idf, lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "450e0ed4",
      "metadata": {
        "id": "450e0ed4",
        "outputId": "e1a0e504-655d-49d6-dc70-cfd8e25958eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "model = pipeline.fit(df_filtered)\n",
        "# Always save it\n",
        "model.write().overwrite().save(\"/user/****/sentiment_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37a3d3be",
      "metadata": {
        "id": "37a3d3be",
        "outputId": "40a45218-6146-4778-9752-5df15a4a834b",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 663:======================================================>(67 + 1) / 68]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------------------------------------------------------+----------+-------------------------------------------------------------+\n",
            "|Article_title                                                                                              |prediction|probability                                                  |\n",
            "+-----------------------------------------------------------------------------------------------------------+----------+-------------------------------------------------------------+\n",
            "|IJR, ONTO, FN, RMBS: ETF Outflow Alert                                                                     |1.0       |[0.026614211124463626,0.7877766447745989,0.1856091441009375] |\n",
            "|After Hours Most Active for Aug 11, 2021 : CLOV, HBAN, CVE, QQQ, ACWI, CLF, MDLZ, KBWB, MTG, MFC, ABBV, LXP|1.0       |[0.08136809291672205,0.7806137080332065,0.13801819905007157] |\n",
            "|Weekly Economic Overview (September 3 - 6, 2013).                                                          |1.0       |[0.041916039498194585,0.5945974270876795,0.363486533414126]  |\n",
            "|Why Arcimoto Stock Skyrocketed 721.7% in 2020                                                              |1.0       |[0.11606102949725222,0.4898264555454471,0.39411251495730076] |\n",
            "|TJX Companies, Inc. (TJX) Ex-Dividend Date Scheduled for November 10, 2014                                 |1.0       |[0.03866040843228367,0.6616713993801163,0.2996681921876]     |\n",
            "|Notable ETF Inflow Detected - IGV, ADBE, CRM, INTU                                                         |1.0       |[0.07461429951010871,0.7304744001967016,0.19491130029318962] |\n",
            "|Up 50% in the Past Month, Is Beyond Meat Stock a Buy?                                                      |1.0       |[0.07610443547426426,0.5582352572368176,0.3656603072889182]  |\n",
            "|Hercules Technology (HTGC) Q4 Earnings and Revenues Beat Estimates                                         |1.0       |[0.05063247691976121,0.5230022872212867,0.426365235858952]   |\n",
            "|CONSOL Lags EPS, Issues Guidance - Analyst Blog                                                            |1.0       |[0.007977603765812413,0.6018319169518279,0.39019047928235967]|\n",
            "|Noteworthy Wednesday Option Activity: WBA, CREE, ITCI                                                      |2.0       |[0.006966907529717881,0.4627143789590284,0.5303187135112538] |\n",
            "+-----------------------------------------------------------------------------------------------------------+----------+-------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# model = PipelineModel.load(\"/user/****/sentiment_model\")\n",
        "\n",
        "df_with_prediction = model.transform(df_filtered)\n",
        "\n",
        "df_with_prediction.select(\"Article_title\", \"prediction\", \"probability\").show(10, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d140a1bd",
      "metadata": {
        "id": "d140a1bd"
      },
      "source": [
        "## Testing with Price Data\n",
        "\n",
        "In order to evaluat our sentiment analysis, we will load historical daily price data for the symbols.\n",
        "\n",
        "I have downloaded price data from https://github.com/Zdong104/FNSPID_Financial_News_Dataset?tab=readme-ov-file and injested into HDFS. It is a folder containing **7694 price files of thousands of rows**. Each file is named by the stock symbol like below. After inspection I will load all csv files at once with manual schema because it is faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5de06436",
      "metadata": {
        "id": "5de06436",
        "outputId": "c9deba30-3f02-4b52-b8da-71a4aadd1be7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+------------------+------------------+------------------+------------------+------------------+--------+\n",
            "|      date|              open|              high|               low|             close|         adj close|  volume|\n",
            "+----------+------------------+------------------+------------------+------------------+------------------+--------+\n",
            "|2023-12-28|194.13999938964844| 194.6600036621093| 193.1699981689453| 193.5800018310547| 193.5800018310547|34014500|\n",
            "|2023-12-27| 192.4900054931641|             193.5|191.08999633789065| 193.1499938964844| 193.1499938964844|48087700|\n",
            "|2023-12-26| 193.6100006103516|193.88999938964844| 192.8300018310547| 193.0500030517578| 193.0500030517578|28919300|\n",
            "|2023-12-22|195.17999267578125| 195.4100036621093|192.97000122070312| 193.6000061035156| 193.6000061035156|37122800|\n",
            "|2023-12-21| 196.1000061035156| 197.0800018310547|             193.5|194.67999267578125|194.67999267578125|46482500|\n",
            "+----------+------------------+------------------+------------------+------------------+------------------+--------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- date: date (nullable = true)\n",
            " |-- open: double (nullable = true)\n",
            " |-- high: double (nullable = true)\n",
            " |-- low: double (nullable = true)\n",
            " |-- close: double (nullable = true)\n",
            " |-- adj close: double (nullable = true)\n",
            " |-- volume: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_price = spark.read.csv(\"hdfs:///user/****/full_history/AAPL.csv\", header=True, inferSchema=True)\n",
        "df_price.show(5)\n",
        "df_price.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f139a2ef",
      "metadata": {
        "id": "f139a2ef",
        "outputId": "1cf6463a-6a9e-423c-d0ec-38eafcd1c6fc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "schema = StructType([\n",
        "    StructField(\"Date\", StringType(), True),\n",
        "    StructField(\"Open\", DoubleType(), True),\n",
        "    StructField(\"High\", DoubleType(), True),\n",
        "    StructField(\"Low\", DoubleType(), True),\n",
        "    StructField(\"Close\", DoubleType(), True),\n",
        "    StructField(\"Adj Close\", DoubleType(), True),\n",
        "    StructField(\"Volume\", LongType(), True)\n",
        "])\n",
        "\n",
        "# Read all the csvs, they will be concat together\n",
        "price_df = spark.read.csv(\"/user/****/full_history/*.csv\", header=True, schema=schema)\n",
        "\n",
        "price_df = price_df.withColumn(\"Date\", to_date(\"Date\", \"yyyy-MM-dd\"))\n",
        "\n",
        "# https://stackoverflow.com/questions/68915138/spark-sql-regex-to-extract-date-file-name-and-brand\n",
        "price_df = price_df.withColumn(\n",
        "    \"Stock_symbol\",\n",
        "    regexp_extract(input_file_name(), r\"([^/]+)\\.csv$\", 1)  # grabs the symbol from path\n",
        ")\n",
        "\n",
        "# Select only needed columns\n",
        "price_df = price_df.select(\"Date\", \"Stock_symbol\", col(\"Adj Close\").alias(\"adj_close\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daeacabb",
      "metadata": {
        "id": "daeacabb"
      },
      "source": [
        "To test our sentiment prediction, we will take a 1 day long/short position on the selected symbol if the prediction is 2/0 respectively. To compute the return, we will compute future 1 day return: (Price(t+1)-Price(t))/Price(t)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81f0cddc",
      "metadata": {
        "id": "81f0cddc"
      },
      "outputs": [],
      "source": [
        "window_spec = Window.partitionBy(\"Stock_symbol\").orderBy(\"Date\")\n",
        "\n",
        "# Shift the price by 1\n",
        "price_df = price_df.withColumn(\n",
        "    \"adj_close_plus_1\", lead(\"adj_close\", 1).over(window_spec)\n",
        ")\n",
        "\n",
        "# Compute the return\n",
        "price_df = price_df.withColumn(\n",
        "    \"return\",\n",
        "    (col(\"adj_close_plus_1\") - col(\"adj_close\")) / col(\"adj_close\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d9be755",
      "metadata": {
        "id": "3d9be755"
      },
      "source": [
        "Now we take a look at the tables. We want to join them on Stock_symbol and Date. For each artical, we will have the forward 1 day return corresponding to the symbol and date."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e41c7f4e",
      "metadata": {
        "id": "e41c7f4e",
        "outputId": "0162d14f-8720-443f-e1a8-fcd80e333a37"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+------------+------------------+------------------+--------------------+\n",
            "|      Date|Stock_symbol|         adj_close|  adj_close_plus_1|              return|\n",
            "+----------+------------+------------------+------------------+--------------------+\n",
            "|1962-01-02|          AA|1.5366575717926023|1.5602117776870728|0.015328207355262016|\n",
            "|1962-01-03|          AA|1.5602117776870728|1.5602117776870728|                 0.0|\n",
            "|1962-01-04|          AA|1.5602117776870728|1.5583264827728271|-0.00120835834032...|\n",
            "|1962-01-05|          AA|1.5583264827728271|1.5074503421783447|-0.03264793427880103|\n",
            "|1962-01-08|          AA|1.5074503421783447|1.4952024221420288|-0.00812492437967...|\n",
            "|1962-01-09|          AA|1.4952024221420288|1.4970871210098269|0.001260497468361...|\n",
            "|1962-01-10|          AA|1.4970871210098269|1.4914336204528809|-0.00377633370670...|\n",
            "|1962-01-11|          AA|1.4914336204528809|1.4603431224822998|-0.02084604875753054|\n",
            "|1962-01-12|          AA|1.4603431224822998| 1.424540400505066|-0.02451665052277...|\n",
            "|1962-01-15|          AA| 1.424540400505066|1.4132344722747805|-0.00793654446464...|\n",
            "+----------+------------+------------------+------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 680:======================================================>(67 + 1) / 68]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+------------+--------------------+----------+\n",
            "|                Date|Stock_symbol|       Article_title|prediction|\n",
            "+--------------------+------------+--------------------+----------+\n",
            "|2023-12-13 00:00:...|       OXSQZ|Lantheus Holdings...|       1.0|\n",
            "|2018-01-12 00:00:...|         WDC|Zacks.com feature...|       2.0|\n",
            "|2017-08-22 00:00:...|         PSA|Public Storage's ...|       1.0|\n",
            "|2023-12-15 00:00:...|       PAVMZ|Celanese (CE) Ral...|       1.0|\n",
            "|2021-03-26 00:00:...|          AA|Notable Friday Op...|       2.0|\n",
            "|2022-05-30 00:00:...|         ADM|Should iShares Ru...|       2.0|\n",
            "|2023-10-19 00:00:...|        ESEB|READ: Rosenbluth ...|       1.0|\n",
            "|2016-06-17 00:00:...|         PEP|New Philadelphia ...|       2.0|\n",
            "|2022-02-07 00:00:...|        NDSN|Graham (GHM) Repo...|       1.0|\n",
            "|2010-12-13 00:00:...|        ZUMZ|This Quarter's To...|       2.0|\n",
            "+--------------------+------------+--------------------+----------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Two table that we are joining\n",
        "price_df.show(10)\n",
        "\n",
        "# This show triggers a big evaluation\n",
        "df_with_prediction.limit(10).select(\"Date\",\"Stock_symbol\",\"Article_title\",\"prediction\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c44c01e3",
      "metadata": {
        "id": "c44c01e3"
      },
      "outputs": [],
      "source": [
        "# Join on symbol and date\n",
        "joined_df = df_with_prediction.join(\n",
        "    price_df,\n",
        "    on=[\"Date\", \"Stock_symbol\"],\n",
        "    how=\"left\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b33284f",
      "metadata": {
        "id": "0b33284f",
        "outputId": "948affad-5d89-4d39-f873-30448c431752"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 688:>                                                        (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+------------+--------------------+----------+--------------------+\n",
            "|                Date|Stock_symbol|       Article_title|prediction|              return|\n",
            "+--------------------+------------+--------------------+----------+--------------------+\n",
            "|2022-05-10 00:00:...|          AA|Should iShares Co...|       2.0|0.014684934419002848|\n",
            "|2023-02-09 00:00:...|         ACB|Aurora Cannabis I...|       2.0|-0.01086955465418...|\n",
            "|2021-08-31 00:00:...|         ACI|Noteworthy ETF In...|       1.0|-0.00592886368765...|\n",
            "|2023-11-29 00:00:...|         ACT|Everest Group (EG...|       2.0|0.004349363219322416|\n",
            "|2012-09-12 00:00:...|        ACTG|Zacks #1 Rank Add...|       1.0| 0.02628707597191735|\n",
            "|2015-06-24 00:00:...|         AGI|Monsanto Tops Q3 ...|       1.0|-0.01733085329818...|\n",
            "|2023-08-04 00:00:...|        AGIO|Agios (AGIO) Q2 E...|       0.0|0.003534962915327802|\n",
            "|2023-12-07 00:00:...|       AGM-A|Why Is Inter Parf...|       1.0|                 0.0|\n",
            "|2023-12-11 00:00:...|       AGM-A|2 Unstoppable Gro...|       1.0|                 0.0|\n",
            "|2023-12-15 00:00:...|       AGM-A|Carnival Stock Ha...|       1.0|-0.01716141320192...|\n",
            "+--------------------+------------+--------------------+----------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "joined_df.filter(col(\"return\").isNotNull()).select(\"Date\", \"Stock_symbol\", \"Article_title\", \"prediction\", \"return\").show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c749bbef",
      "metadata": {
        "id": "c749bbef",
        "outputId": "938b3ae9-b97e-4c81-d9bd-cb4605ca4e0d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 694:>                                                        (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+------------+--------------------+----------+--------------------+--------------------+\n",
            "|                Date|Stock_symbol|       Article_title|prediction|              return|     strategy_return|\n",
            "+--------------------+------------+--------------------+----------+--------------------+--------------------+\n",
            "|2018-10-31 00:00:...|         ACB|It Took Just 1 We...|       0.0|-0.01029407296076...|0.010294072960761444|\n",
            "|2018-11-27 00:00:...|         ACB|Now That the Pot ...|       2.0| 0.08363631277373343| 0.08363631277373343|\n",
            "|2019-06-04 00:00:...|         ACB|Absent Immediate ...|       1.0|-0.01778907519848998|                 0.0|\n",
            "|2019-08-28 00:00:...|         ACB|Despite Pot Stock...|       1.0|-0.00537635327469...|                 0.0|\n",
            "|2019-11-12 00:00:...|         ACM|AECOM Technology ...|       1.0|-0.02018090743603...|                 0.0|\n",
            "|2022-04-19 00:00:...|        ACMR|Validea Peter Lyn...|       2.0|-0.01525822857467...|-0.01525822857467...|\n",
            "|2016-03-23 00:00:...|         ADI|Analog Devices to...|       2.0|0.005314507015057924|0.005314507015057924|\n",
            "|2017-07-07 00:00:...|         ADI|Microsemi Rating ...|       2.0|0.003313426435224797|0.003313426435224797|\n",
            "|2016-06-07 00:00:...|         ADP|Automatic Data Pr...|       1.0|0.010192086984231952|                 0.0|\n",
            "|2013-12-18 00:00:...|        ADUS|Tenet Downgraded ...|       1.0|-0.03350292666390068|                 0.0|\n",
            "+--------------------+------------+--------------------+----------+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "df_with_strategy_return = joined_df.filter(col(\"return\").isNotNull())\\\n",
        "                                   .withColumn(\n",
        "                                        \"strategy_return\",\n",
        "                                        when(col(\"prediction\") == 2.0, col(\"return\"))\n",
        "                                        .when(col(\"prediction\") == 0.0, -col(\"return\"))\n",
        "                                        .otherwise(0.0)\n",
        "                                    )\n",
        "df_with_strategy_return.select(\"Date\", \"Stock_symbol\", \"Article_title\", \"prediction\", \"return\", \"strategy_return\").show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "276bc1d8",
      "metadata": {
        "id": "276bc1d8",
        "outputId": "8eb7e0a0-0495-495d-deff-d4cfaf2084c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+--------------------+--------------------+--------------------+------------------+------------------+-----------+--------------+------------+\n",
            "|   n|         mean_return|          std_return|          min_return|        max_return|      total_return|long_counts|neutral_counts|short_counts|\n",
            "+----+--------------------+--------------------+--------------------+------------------+------------------+-----------+--------------+------------+\n",
            "|1382|5.250920769828881E-4|0.025672836615400586|-0.17098445749610666|0.7562499940395355|0.7256772503903514|        366|           897|         119|\n",
            "+----+--------------------+--------------------+--------------------+------------------+------------------+-----------+--------------+------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 709:=============================================>           (4 + 1) / 5]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Approx. Median strategy return: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "stats = df_with_strategy_return.select(\n",
        "    count(\"strategy_return\").alias(\"n\"),\n",
        "    mean(\"strategy_return\").alias(\"mean_return\"),\n",
        "    stddev(\"strategy_return\").alias(\"std_return\"),\n",
        "    min(\"strategy_return\").alias(\"min_return\"),\n",
        "    max(\"strategy_return\").alias(\"max_return\"),\n",
        "    sum(\"strategy_return\").alias(\"total_return\"),\n",
        "    sum(when(col(\"prediction\") == 2.0, 1).otherwise(0)).alias(\"long_counts\"),\n",
        "    sum(when(col(\"prediction\") == 1.0, 1).otherwise(0)).alias(\"neutral_counts\"),\n",
        "    sum(when(col(\"prediction\") == 0.0, 1).otherwise(0)).alias(\"short_counts\")\n",
        ")\n",
        "\n",
        "stats.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78ae464b",
      "metadata": {
        "id": "78ae464b"
      },
      "source": [
        "## Results\n",
        "number of articles = 10k \\\n",
        "n (number of trades) = 1382 \\\n",
        "mean (average return) = 5.25e-4 \\\n",
        "stdev of return = 0.03 \\\n",
        "min return = -0.17 \\\n",
        "max max = 0.75 \\\n",
        "sum(return) = 0.75\n",
        "\n",
        "The average return is near 0 but positive meaning we have a tiny edge. However, we get near 0 if we normalize the return by standard deviation. This is the Sharpe ratio, and it is saying that our return per unit risk is very low.\n",
        "\n",
        "The sum of the return is 75% meaning that if we invest a fix dollar amount on each prediction, our cumulative return is 75%.\n",
        "\n",
        "The results indicate that the data pipeline is working. We can proceed to train a full-size model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "119b10df",
      "metadata": {
        "id": "119b10df"
      },
      "source": [
        "# Training Bigger Sample Size and Seperate Test Set\n",
        "\n",
        "\n",
        "Steps are the same as above without intermediate triggering action. I am using all 15M articles instead of 10k in the example above. After the initial filter, we have rouhgly 2M. I am also splitting training and testing set in a 80-20 split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf80bcd3",
      "metadata": {
        "id": "cf80bcd3",
        "outputId": "a915b901-e4e0-46f6-d01e-ae07b5222862"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1993258\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 126:======================================================>(67 + 1) / 68]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "498520\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Reduce sample size and drop empty\n",
        "df = spark.read.parquet('/user/****/nasdaq_parquet/')\n",
        "\n",
        "# Train-test split\n",
        "train_df, test_df = df.randomSplit([0.8, 0.2])\n",
        "\n",
        "# Filtering Training\n",
        "train_df_filtered = (\n",
        "    train_df.dropna(subset=[\"Article\", \"Stock_symbol\"])  # removes nulls\n",
        "      .filter(\n",
        "          (col(\"Article\").isNotNull()) & (col(\"Article\") != \"\") &\n",
        "          (col(\"Stock_symbol\").isNotNull()) & (col(\"Stock_symbol\") != \"\")\n",
        "      )\n",
        ")\n",
        "\n",
        "# Filtering Test\n",
        "test_df_filtered = (\n",
        "    test_df.dropna(subset=[\"Article\", \"Stock_symbol\"])  # removes nulls\n",
        "      .filter(\n",
        "          (col(\"Article\").isNotNull()) & (col(\"Article\") != \"\") &\n",
        "          (col(\"Stock_symbol\").isNotNull()) & (col(\"Stock_symbol\") != \"\")\n",
        "      )\n",
        ")\n",
        "\n",
        "# Count just 1 column, it's columnar storage so this is faster but still takes a while\n",
        "print(train_df_filtered.select(\"Date\").count())\n",
        "print(test_df_filtered.select(\"Date\").count())\n",
        "\n",
        "train_df_filtered = train_df_filtered.withColumn(\"scaled_sentiment_score\", score_udf(col(\"Article_title\"))) \\\n",
        "                                     .withColumn(\"label\", when(col(\"scaled_sentiment_score\") > 0, 2)\n",
        "                                                         .when(col(\"scaled_sentiment_score\") == 0, 1)\n",
        "                                                         .when(col(\"scaled_sentiment_score\") < 0, 0)\n",
        "                                                         .otherwise(None))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8181a13a",
      "metadata": {
        "id": "8181a13a",
        "outputId": "340565d0-c246-4fad-a5b1-665974499ee0",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Labeling\n",
        "\n",
        "\n",
        "# Featurizing Article, all discussed above\n",
        "tokenizer = Tokenizer(inputCol=\"Article\", outputCol=\"words\")\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "tf = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=200)\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", family=\"multinomial\", maxIter=100)\n",
        "pipeline = Pipeline(stages=[tokenizer, remover, tf, idf, lr])\n",
        "\n",
        "# Fitting\n",
        "model = pipeline.fit(train_df_filtered)\n",
        "# Always save it\n",
        "model.write().overwrite().save(\"/user/****/sentiment_model_big\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10526ced",
      "metadata": {
        "id": "10526ced",
        "outputId": "3332a97f-eab7-4212-f60a-7951e23a5587"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>n</th>\n",
              "      <th>mean_return</th>\n",
              "      <th>std_return</th>\n",
              "      <th>min_return</th>\n",
              "      <th>max_return</th>\n",
              "      <th>total_return</th>\n",
              "      <th>long_counts</th>\n",
              "      <th>neutral_counts</th>\n",
              "      <th>short_counts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1636617</td>\n",
              "      <td>0.000148</td>\n",
              "      <td>0.205479</td>\n",
              "      <td>-1.348358</td>\n",
              "      <td>259.095241</td>\n",
              "      <td>242.897535</td>\n",
              "      <td>306328</td>\n",
              "      <td>1316561</td>\n",
              "      <td>13728</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         n  mean_return  std_return  min_return  max_return  total_return  \\\n",
              "0  1636617     0.000148    0.205479   -1.348358  259.095241    242.897535   \n",
              "\n",
              "   long_counts  neutral_counts  short_counts  \n",
              "0       306328         1316561         13728  "
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Evaluation on Train set\n",
        "train_df_with_prediction = model.transform(train_df_filtered)\n",
        "\n",
        "train_joined_df = train_df_with_prediction.join(\n",
        "    price_df,\n",
        "    on=[\"Date\", \"Stock_symbol\"],\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "train_df_with_strategy_return = train_joined_df.filter(col(\"return\").isNotNull())\\\n",
        "                                         .withColumn(\n",
        "                                                \"strategy_return\",\n",
        "                                                when(col(\"prediction\") == 2.0, col(\"return\"))\n",
        "                                                .when(col(\"prediction\") == 0.0, -col(\"return\"))\n",
        "                                                .otherwise(0.0))\n",
        "\n",
        "stats = train_df_with_strategy_return.select(\n",
        "    count(\"strategy_return\").alias(\"n\"),\n",
        "    mean(\"strategy_return\").alias(\"mean_return\"),\n",
        "    stddev(\"strategy_return\").alias(\"std_return\"),\n",
        "    min(\"strategy_return\").alias(\"min_return\"),\n",
        "    max(\"strategy_return\").alias(\"max_return\"),\n",
        "    sum(\"strategy_return\").alias(\"total_return\"),\n",
        "    sum(when(col(\"prediction\") == 2.0, 1).otherwise(0)).alias(\"long_counts\"),\n",
        "    sum(when(col(\"prediction\") == 1.0, 1).otherwise(0)).alias(\"neutral_counts\"),\n",
        "    sum(when(col(\"prediction\") == 0.0, 1).otherwise(0)).alias(\"short_counts\")\n",
        ")\n",
        "\n",
        "stats.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff064e7d",
      "metadata": {
        "id": "ff064e7d",
        "outputId": "75efa025-5d09-46e9-863a-6d3a13209939"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>n</th>\n",
              "      <th>mean_return</th>\n",
              "      <th>std_return</th>\n",
              "      <th>min_return</th>\n",
              "      <th>max_return</th>\n",
              "      <th>total_return</th>\n",
              "      <th>long_counts</th>\n",
              "      <th>neutral_counts</th>\n",
              "      <th>short_counts</th>\n",
              "      <th>correct_long</th>\n",
              "      <th>false_long</th>\n",
              "      <th>correct_short</th>\n",
              "      <th>false_short</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>409396</td>\n",
              "      <td>0.00009</td>\n",
              "      <td>0.058472</td>\n",
              "      <td>-0.962185</td>\n",
              "      <td>35.4625</td>\n",
              "      <td>36.941146</td>\n",
              "      <td>76492</td>\n",
              "      <td>329530</td>\n",
              "      <td>3374</td>\n",
              "      <td>37471</td>\n",
              "      <td>36003</td>\n",
              "      <td>1657</td>\n",
              "      <td>1670</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        n  mean_return  std_return  min_return  max_return  total_return  \\\n",
              "0  409396      0.00009    0.058472   -0.962185     35.4625     36.941146   \n",
              "\n",
              "   long_counts  neutral_counts  short_counts  correct_long  false_long  \\\n",
              "0        76492          329530          3374         37471       36003   \n",
              "\n",
              "   correct_short  false_short  \n",
              "0           1657         1670  "
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Evaluation on Test set\n",
        "model = PipelineModel.load(\"/user/****/sentiment_model_big\")\n",
        "\n",
        "test_df_with_prediction = model.transform(test_df_filtered)\n",
        "\n",
        "test_joined_df = test_df_with_prediction.join(\n",
        "    price_df,\n",
        "    on=[\"Date\", \"Stock_symbol\"],\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "test_df_with_strategy_return = test_joined_df.filter(col(\"return\").isNotNull())\\\n",
        "                                         .withColumn(\n",
        "                                                \"strategy_return\",\n",
        "                                                when(col(\"prediction\") == 2.0, col(\"return\"))\n",
        "                                                .when(col(\"prediction\") == 0.0, -col(\"return\"))\n",
        "                                                .otherwise(0.0))\n",
        "\n",
        "stats = test_df_with_strategy_return.select(\n",
        "    count(\"strategy_return\").alias(\"n\"),\n",
        "    mean(\"strategy_return\").alias(\"mean_return\"),\n",
        "    stddev(\"strategy_return\").alias(\"std_return\"),\n",
        "    min(\"strategy_return\").alias(\"min_return\"),\n",
        "    max(\"strategy_return\").alias(\"max_return\"),\n",
        "    spark_sum(\"strategy_return\").alias(\"total_return\"),\n",
        "\n",
        "    # Prediction counts\n",
        "    spark_sum(when(col(\"prediction\") == 2.0, 1).otherwise(0)).alias(\"long_counts\"),\n",
        "    spark_sum(when(col(\"prediction\") == 1.0, 1).otherwise(0)).alias(\"neutral_counts\"),\n",
        "    spark_sum(when(col(\"prediction\") == 0.0, 1).otherwise(0)).alias(\"short_counts\"),\n",
        "\n",
        "    # Long success/failure\n",
        "    spark_sum(when((col(\"prediction\") == 2.0) & (col(\"return\") > 0), 1).otherwise(0)).alias(\"correct_long\"),\n",
        "    spark_sum(when((col(\"prediction\") == 2.0) & (col(\"return\") < 0), 1).otherwise(0)).alias(\"false_long\"),\n",
        "\n",
        "    # Short success/failure\n",
        "    spark_sum(when((col(\"prediction\") == 0.0) & (col(\"return\") < 0), 1).otherwise(0)).alias(\"correct_short\"),\n",
        "    spark_sum(when((col(\"prediction\") == 0.0) & (col(\"return\") > 0), 1).otherwise(0)).alias(\"false_short\")\n",
        ")\n",
        "\n",
        "stats.toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab6f523c",
      "metadata": {
        "id": "ab6f523c"
      },
      "source": [
        "## Train Results\n",
        "number of articles = 2M \\\n",
        "n (number of trades) = 409k \\\n",
        "mean (average return) = 9e-5 \\\n",
        "stdev of return = 0.058 \\\n",
        "min return = -0.96 \\\n",
        "max max = 35.46 \\\n",
        "sum(return) = 36.94\n",
        "\n",
        "Long correct= 37471 / (37471+36003) = 51%\n",
        "Short correction = 1657 / (1657+1670) = 49.8%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0686c238",
      "metadata": {
        "id": "0686c238",
        "outputId": "9bd09127-12bf-4455-953c-6b958a50f7a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>n</th>\n",
              "      <th>mean_return</th>\n",
              "      <th>std_return</th>\n",
              "      <th>min_return</th>\n",
              "      <th>max_return</th>\n",
              "      <th>total_return</th>\n",
              "      <th>long_counts</th>\n",
              "      <th>neutral_counts</th>\n",
              "      <th>short_counts</th>\n",
              "      <th>correct_long</th>\n",
              "      <th>false_long</th>\n",
              "      <th>correct_short</th>\n",
              "      <th>false_short</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>408404</td>\n",
              "      <td>-0.000052</td>\n",
              "      <td>0.018741</td>\n",
              "      <td>-1.348358</td>\n",
              "      <td>4.612245</td>\n",
              "      <td>-21.287422</td>\n",
              "      <td>76834</td>\n",
              "      <td>328131</td>\n",
              "      <td>3439</td>\n",
              "      <td>37636</td>\n",
              "      <td>36281</td>\n",
              "      <td>1676</td>\n",
              "      <td>1705</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        n  mean_return  std_return  min_return  max_return  total_return  \\\n",
              "0  408404    -0.000052    0.018741   -1.348358    4.612245    -21.287422   \n",
              "\n",
              "   long_counts  neutral_counts  short_counts  correct_long  false_long  \\\n",
              "0        76834          328131          3439         37636       36281   \n",
              "\n",
              "   correct_short  false_short  \n",
              "0           1676         1705  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Evaluation on Test set\n",
        "\n",
        "model = PipelineModel.load(\"/user/****/sentiment_model_big\")\n",
        "\n",
        "test_df_with_prediction = model.transform(test_df_filtered)\n",
        "\n",
        "test_joined_df = test_df_with_prediction.join(\n",
        "    price_df,\n",
        "    on=[\"Date\", \"Stock_symbol\"],\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "test_df_with_strategy_return = test_joined_df.filter(col(\"return\").isNotNull())\\\n",
        "                                         .withColumn(\n",
        "                                                \"strategy_return\",\n",
        "                                                when(col(\"prediction\") == 2.0, col(\"return\"))\n",
        "                                                .when(col(\"prediction\") == 0.0, -col(\"return\"))\n",
        "                                                .otherwise(0.0))\n",
        "\n",
        "stats = test_df_with_strategy_return.select(\n",
        "    count(\"strategy_return\").alias(\"n\"),\n",
        "    mean(\"strategy_return\").alias(\"mean_return\"),\n",
        "    stddev(\"strategy_return\").alias(\"std_return\"),\n",
        "    min(\"strategy_return\").alias(\"min_return\"),\n",
        "    max(\"strategy_return\").alias(\"max_return\"),\n",
        "    spark_sum(\"strategy_return\").alias(\"total_return\"),\n",
        "\n",
        "    # Prediction counts\n",
        "    spark_sum(when(col(\"prediction\") == 2.0, 1).otherwise(0)).alias(\"long_counts\"),\n",
        "    spark_sum(when(col(\"prediction\") == 1.0, 1).otherwise(0)).alias(\"neutral_counts\"),\n",
        "    spark_sum(when(col(\"prediction\") == 0.0, 1).otherwise(0)).alias(\"short_counts\"),\n",
        "\n",
        "    # Long success/failure\n",
        "    spark_sum(when((col(\"prediction\") == 2.0) & (col(\"return\") > 0), 1).otherwise(0)).alias(\"correct_long\"),\n",
        "    spark_sum(when((col(\"prediction\") == 2.0) & (col(\"return\") < 0), 1).otherwise(0)).alias(\"false_long\"),\n",
        "\n",
        "    # Short success/failure\n",
        "    spark_sum(when((col(\"prediction\") == 0.0) & (col(\"return\") < 0), 1).otherwise(0)).alias(\"correct_short\"),\n",
        "    spark_sum(when((col(\"prediction\") == 0.0) & (col(\"return\") > 0), 1).otherwise(0)).alias(\"false_short\")\n",
        ")\n",
        "\n",
        "stats.toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6437b621",
      "metadata": {
        "id": "6437b621"
      },
      "source": [
        "## Test Results\n",
        "number of articles = 498k \\\n",
        "n (number of trades) = 408k \\\n",
        "mean (average return) = -5.2e-5 \\\n",
        "stdev of return = 0.018 \\\n",
        "min return = -1.34 \\\n",
        "max max = 4.61 \\\n",
        "sum(return) = -21.28\n",
        "\n",
        "Long correct= 37636 / (37636+36281) = 50.9%\n",
        "Short correction = 1676 / (1676+1705) = 49.6%\n",
        "\n",
        "Long and short correction rates are similar to those of training. This means our model isn't overfitting. The actual returns are less conclusive. Both the test and train results have near 0 average return and 0 Sharpe ratio (average/stdev).\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This project aims to utilize the big data tools that we learned to build a custom machine learning pipeline in sentiment driven trading. Based on the results, it appears that extracting sentiments from news is not the most reliable approach. We are not sure sentiments themselves have sufficient values in making trading decisions. However, it was still a valuable project that proves to be capable of handling massive data in a practical example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0238a7ce",
      "metadata": {
        "id": "0238a7ce"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
